{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd6ad0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a33009",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_network:\n",
    "\n",
    "    def __init__(self, data, architecture:list=[2,3,3,1], activation_function:str=\"sigmoid\"): #initial test arch\n",
    "        # TODO: PROVIDE VALIDATION TO DATA\n",
    "        self.data = data\n",
    "\n",
    "        self.architecture = architecture\n",
    "        \n",
    "        # weights initialized using random numbers from normal distribution\n",
    "        self.weights = [\n",
    "            np.random.randn(architecture[x], architecture[x+1])\n",
    "            for x in range(len(architecture)-1)\n",
    "            ]\n",
    "        \n",
    "        # biases initialized using zeros for each neuron\n",
    "        self.biases = [\n",
    "            np.zeros(architecture[x+1])\n",
    "            for x in range(len(architecture)-1)\n",
    "        ]\n",
    "        \n",
    "        # activation functions supported\n",
    "        activation_functions_available = {\n",
    "            \"relu\": lambda x: np.maximum(0, x),\n",
    "            \"tanh\": np.tanh,\n",
    "            \"sigmoid\": lambda x: ((1) / (1 + np.exp(-x)))\n",
    "        }\n",
    "\n",
    "        # validation of the activation function\n",
    "        if activation_function.lower() in [\"relu\", \"tanh\", \"sigmoid\"]:\n",
    "            self.activation_function = activation_functions_available[activation_function.lower()]\n",
    "        else: \n",
    "            raise NameError(\"Activation Function not supported\")\n",
    "\n",
    "    def _forward_propagation(self):\n",
    "        \"\"\"\n",
    "        Basic formula:\n",
    "        Z[l] = W[l] A[l-1] + b[l]\n",
    "        A[l] = g(Z[l])\n",
    "        \n",
    "        Where:\n",
    "            - l: Current Layer\n",
    "            - W: Weights\n",
    "            - A: Activation Vector\n",
    "            - b: Biases\n",
    "            - g: Activation Function\n",
    "        \"\"\"\n",
    "        A = self.data\n",
    "        for layer_idx in range(len(self.architecture)):\n",
    "            Z = self.weights[layer_idx] @ A + self.biases[layer_idx]\n",
    "            A = self.activation_function(Z)\n",
    "        y_hat = A\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63001545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17808417, 0.17808417, 0.17808417, 0.17808417, 0.17808417,\n",
       "        0.17808417, 0.17808417, 0.17808417, 0.17808417, 0.17808417]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. create network architecture\n",
    "L = 3\n",
    "n = [2, 3, 3, 1]\n",
    "\n",
    "# 2. create weights and biases\n",
    "W1 = np.random.randn(n[1], n[0])\n",
    "W2 = np.random.randn(n[2], n[1])\n",
    "W3 = np.random.randn(n[3], n[2])\n",
    "b1 = np.random.randn(n[1], 1)\n",
    "b2 = np.random.randn(n[2], 1)\n",
    "b3 = np.random.randn(n[3], 1)\n",
    "\n",
    "# 3. create training data and labels\n",
    "def prepare_data():\n",
    "  X = np.array([\n",
    "      [150, 70],\n",
    "      [254, 73],\n",
    "      [312, 68],\n",
    "      [120, 60],\n",
    "      [154, 61],\n",
    "      [212, 65],\n",
    "      [216, 67],\n",
    "      [145, 67],\n",
    "      [184, 64],\n",
    "      [130, 69]\n",
    "  ])\n",
    "  y = np.array([0,1,1,0,0,1,1,0,1,0])\n",
    "  m = 10\n",
    "  A0 = X.T\n",
    "  Y = y.reshape(n[L], m)\n",
    "\n",
    "  return A0, Y\n",
    "\n",
    "# 4. create activation function\n",
    "def sigmoid(arr):\n",
    "  return 1 / (1 + np.exp(-1 * arr))\n",
    "\n",
    "# 5. create feed forward process\n",
    "def feed_forward(A0):\n",
    "\n",
    "  # layer 1 calculations\n",
    "  Z1 = W1 @ A0 + b1\n",
    "  A1 = sigmoid(Z1)\n",
    "\n",
    "  # layer 2 calculations\n",
    "  Z2 = W2 @ A1 + b2\n",
    "  A2 = sigmoid(Z2)\n",
    "\n",
    "  # layer 3 calculations\n",
    "  Z3 = W3 @ A2 + b3\n",
    "  A3 = sigmoid(Z3)\n",
    "  \n",
    "  y_hat = A3\n",
    "  return y_hat\n",
    "\n",
    "A0, Y = prepare_data()\n",
    "y_hat = feed_forward(A0)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41555826",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuron:\n",
    "    # SET CONNECTION HERE? MAY ITERATE EACH NEURON FOR EACH ONE OF THE NEXT LAYER WITH RANDOM UNIFORM NUMBERS...\n",
    "    # IF NEURON1 == NONE -> INPUT NEURONS, IF NEURON2 == NONE -> OUTPUT NEURON\n",
    "    # BIAS ADDITION GOES AFTER WEIGHT MULTIPLICATION\n",
    "    # act_func*w + b\n",
    "    # TO MAKE USE OF MATRICES FOR THE CALCULATIONS\n",
    "    @staticmethod\n",
    "    def _relu(x:float):\n",
    "        return (max(0, x))\n",
    "\n",
    "    @staticmethod\n",
    "    def _tanh(x:float):\n",
    "        return ((np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x)))\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(x):\n",
    "        return ((1) / (1 + np.exp(-x)))\n",
    "    \n",
    "    def __init__(self, neuron_bias:float=None, activation_function:str=\"sigmoid\"):\n",
    "        self.bias = np.random.randn() if neuron_bias is None else neuron_bias\n",
    "        self.activation_functions = {\n",
    "            \"relu\": self._relu,\n",
    "            \"tanh\": self._tanh,\n",
    "            \"sigmoid\": self._sigmoid\n",
    "        }\n",
    "        if activation_function.lower() in [\"relu\", \"tanh\", \"sigmoid\"]:\n",
    "            self.activation_function = self.activation_functions[activation_function.lower()]\n",
    "        else:\n",
    "            raise NameError (\"Função de ativação indisponível\")\n",
    "        self.output = None\n",
    "\n",
    "    def activate(self, inputs, weights, bias):\n",
    "        weighted_inputs_with_bias = np.dot(inputs, weights) + bias\n",
    "        self.output = self.activation_function(weighted_inputs_with_bias)\n",
    "        return self.output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
