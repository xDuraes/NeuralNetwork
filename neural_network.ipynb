{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd6ad0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a33009",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_network:\n",
    "\n",
    "    def __init__(self, architecture:list=[2,3,3,1], activation_function:str=\"sigmoid\", learning_rate=0.01):\n",
    "        self.architecture = architecture\n",
    "        self.cache = dict()\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # weights initialized using random numbers from normal distribution\n",
    "        self.weights = [\n",
    "            np.random.randn(architecture[x+1], architecture[x])\n",
    "            for x in range(len(architecture)-1)\n",
    "            ]\n",
    "        \n",
    "        # biases initialized using zeros for each neuron\n",
    "        self.biases = [\n",
    "            np.zeros((architecture[x+1], 1))\n",
    "            for x in range(len(architecture)-1)\n",
    "        ]\n",
    "        \n",
    "        # activation functions supported\n",
    "        activation_functions_available = {\n",
    "            \"relu\": lambda x: np.maximum(0, x),\n",
    "            \"tanh\": np.tanh,\n",
    "            \"sigmoid\": lambda x: ((1) / (1 + np.exp(-x)))\n",
    "        }\n",
    "\n",
    "        # validation of the activation function\n",
    "        if activation_function.lower() in [\"relu\", \"tanh\", \"sigmoid\"]:\n",
    "            self.activation_function = activation_functions_available[activation_function.lower()]\n",
    "        else: \n",
    "            raise NameError(\"Activation Function not supported\")\n",
    "        \n",
    "    def input_data(self, data, y):\n",
    "        # TODO validation\n",
    "        self.data = data\n",
    "        self.y = y\n",
    "\n",
    "    def _forward_propagation(self):\n",
    "        \"\"\"\n",
    "        Basic formula:\n",
    "        Z[l] = W[l] A[l-1] + b[l]\n",
    "        A[l] = g(Z[l])\n",
    "        \n",
    "        Where:\n",
    "            - l: Current Layer\n",
    "            - W: Weights\n",
    "            - A: Activation Vector\n",
    "            - b: Biases\n",
    "            - g: Activation Function\n",
    "        \"\"\"\n",
    "        # dict to save the values of each layer\n",
    "        A = self.data\n",
    "        self.cache[\"A0\"] = A\n",
    "        for layer_idx in range(len(self.architecture)-1):\n",
    "            Z = self.weights[layer_idx] @ A + self.biases[layer_idx]\n",
    "            A = self.activation_function(Z)\n",
    "            self.cache[f\"A{layer_idx+1}\"] = A\n",
    "        y_hat = A\n",
    "        return y_hat\n",
    "    \n",
    "    def _calculate_loss(self, y_hat):\n",
    "        \"\"\"\n",
    "        Use of cross entropy to calculate the loss\n",
    "\n",
    "        For a single example:\n",
    "            - L(y_hat, y) = -(y * log y_hat + (1 - y) * log(1 - y_hat))\n",
    "\n",
    "        For all training samples:\n",
    "            - C = (1 / m) * sum(L(y_hat, y))\n",
    "        \"\"\"\n",
    "        \n",
    "        # original data and prediction\n",
    "        y = self.y\n",
    "        # y_hat = self._forward_propagation()\n",
    "\n",
    "        # loss calculation based on the matrices\n",
    "        prediction_losses = -((y * np.log(y_hat)) + (1 - y) * np.log(1 - y_hat))\n",
    "\n",
    "        # num of entities extracted to get the global loss\n",
    "        y_total = y_hat.reshape(-1).shape[0]\n",
    "\n",
    "        # global loss\n",
    "        losses_sum = (1 / y_total) * np.sum(prediction_losses, axis=1)\n",
    "\n",
    "        return np.sum(losses_sum)\n",
    "\n",
    "    def _backpropagation(self, y_hat, y, m):\n",
    "        A = y_hat\n",
    "        # alterar A no final\n",
    "\n",
    "        for layer_idx in reversed(range(len(self.weights))):\n",
    "            dC_dZ = (1 / m) * (A - y) #TODO adaptar para todas as derivadas de ativação\n",
    "\n",
    "            dZ_dW = self.cache[f\"A{layer_idx - 1}\"]\n",
    "\n",
    "            dC_dW = dC_dZ @ dZ_dW.T\n",
    "\n",
    "            dC_db = np.sum(dC_dZ, axis=1, keepdims=True)\n",
    "\n",
    "            dZ_dA_back = self.weights[layer_idx]\n",
    "            dC_dA_back = self.weights[layer_idx].T @ dC_dZ\n",
    "\n",
    "            # weights and biases adaptation using gradient descent\n",
    "            # theta = theta - learning rate * slope     (derivative)\n",
    "            self.weights[layer_idx] -= self.learning_rate * dC_dW\n",
    "            self.biases[layer_idx] -= self.learning_rate * dC_db\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d1346e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.64945531 0.67494403 0.80161865 0.64945317 0.67490707 0.6749268\n",
      "  0.67492664 0.6494612  0.67492658 0.64945314]]\n",
      "0.7110418453276943\n"
     ]
    }
   ],
   "source": [
    "nn = neural_network()\n",
    "\n",
    "def prepare_data():\n",
    "  X = np.array([\n",
    "      [150, 70],\n",
    "      [254, 73],\n",
    "      [312, 68],\n",
    "      [120, 60],\n",
    "      [154, 61],\n",
    "      [212, 65],\n",
    "      [216, 67],\n",
    "      [145, 67],\n",
    "      [184, 64],\n",
    "      [130, 69]\n",
    "  ])\n",
    "  y = np.array([0,1,1,0,0,1,1,0,1,0])\n",
    "  m = 10\n",
    "  A0 = X.T\n",
    "  Y = y.reshape(1, m)\n",
    "\n",
    "  return A0, Y, m\n",
    "\n",
    "A0, Y, m = prepare_data()\n",
    "\n",
    "nn.input_data(A0, Y)\n",
    "\n",
    "y_hat = nn._forward_propagation()\n",
    "\n",
    "print(y_hat)\n",
    "\n",
    "loss = nn._calculate_loss(y_hat)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63001545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.90754329, 0.91390799, 0.91390799, 0.88710582, 0.91376846,\n",
       "        0.91390797, 0.91390797, 0.90849836, 0.913906  , 0.86624987]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. create network architecture\n",
    "L = 3\n",
    "n = [2, 3, 3, 1]\n",
    "\n",
    "# 2. create weights and biases\n",
    "W1 = np.random.randn(n[1], n[0])\n",
    "W2 = np.random.randn(n[2], n[1])\n",
    "W3 = np.random.randn(n[3], n[2])\n",
    "b1 = np.random.randn(n[1], 1)\n",
    "b2 = np.random.randn(n[2], 1)\n",
    "b3 = np.random.randn(n[3], 1)\n",
    "\n",
    "# 3. create training data and labels\n",
    "def prepare_data():\n",
    "  X = np.array([\n",
    "      [150, 70],\n",
    "      [254, 73],\n",
    "      [312, 68],\n",
    "      [120, 60],\n",
    "      [154, 61],\n",
    "      [212, 65],\n",
    "      [216, 67],\n",
    "      [145, 67],\n",
    "      [184, 64],\n",
    "      [130, 69]\n",
    "  ])\n",
    "  y = np.array([0,1,1,0,0,1,1,0,1,0])\n",
    "  m = 10\n",
    "  A0 = X.T\n",
    "  Y = y.reshape(n[L], m)\n",
    "\n",
    "  return A0, Y\n",
    "\n",
    "# 4. create activation function\n",
    "def sigmoid(arr):\n",
    "  return 1 / (1 + np.exp(-1 * arr))\n",
    "\n",
    "# 5. create feed forward process\n",
    "def feed_forward(A0):\n",
    "\n",
    "  # layer 1 calculations\n",
    "  Z1 = W1 @ A0 + b1\n",
    "  A1 = sigmoid(Z1)\n",
    "\n",
    "  # layer 2 calculations\n",
    "  Z2 = W2 @ A1 + b2\n",
    "  A2 = sigmoid(Z2)\n",
    "\n",
    "  # layer 3 calculations\n",
    "  Z3 = W3 @ A2 + b3\n",
    "  A3 = sigmoid(Z3)\n",
    "  \n",
    "  y_hat = A3\n",
    "  return y_hat\n",
    "\n",
    "A0, Y = prepare_data()\n",
    "y_hat = feed_forward(A0)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "41555826",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuron:\n",
    "    # SET CONNECTION HERE? MAY ITERATE EACH NEURON FOR EACH ONE OF THE NEXT LAYER WITH RANDOM UNIFORM NUMBERS...\n",
    "    # IF NEURON1 == NONE -> INPUT NEURONS, IF NEURON2 == NONE -> OUTPUT NEURON\n",
    "    # BIAS ADDITION GOES AFTER WEIGHT MULTIPLICATION\n",
    "    # act_func*w + b\n",
    "    # TO MAKE USE OF MATRICES FOR THE CALCULATIONS\n",
    "    @staticmethod\n",
    "    def _relu(x:float):\n",
    "        return (max(0, x))\n",
    "\n",
    "    @staticmethod\n",
    "    def _tanh(x:float):\n",
    "        return ((np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x)))\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(x):\n",
    "        return ((1) / (1 + np.exp(-x)))\n",
    "    \n",
    "    def __init__(self, neuron_bias:float=None, activation_function:str=\"sigmoid\"):\n",
    "        self.bias = np.random.randn() if neuron_bias is None else neuron_bias\n",
    "        self.activation_functions = {\n",
    "            \"relu\": self._relu,\n",
    "            \"tanh\": self._tanh,\n",
    "            \"sigmoid\": self._sigmoid\n",
    "        }\n",
    "        if activation_function.lower() in [\"relu\", \"tanh\", \"sigmoid\"]:\n",
    "            self.activation_function = self.activation_functions[activation_function.lower()]\n",
    "        else:\n",
    "            raise NameError (\"Função de ativação indisponível\")\n",
    "        self.output = None\n",
    "\n",
    "    def activate(self, inputs, weights, bias):\n",
    "        weighted_inputs_with_bias = np.dot(inputs, weights) + bias\n",
    "        self.output = self.activation_function(weighted_inputs_with_bias)\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fde24ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
