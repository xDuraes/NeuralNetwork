{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd6ad0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7670ec01",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "- ADAPT THE HIDDEN AND OUTPUT LAYERS TO BE DIFFERENT\n",
    "- ADD THE OPTION TO ADD WEIGHTS AND BIASES AS WELL AS THE OPTION TO SAVE FROM DISK AND LOAD TO DISK\n",
    "- SET THE LAST LAYER IN THE BACKPROPAGATION DINAMICALLY\n",
    "- SET WAYS TO PREPROCESS DATA TO NOT BREAK THE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a33009",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_network:\n",
    "    # TODO: ADD A WAY TO USE DIFFERENT HIDDEN LAYERS ACTIVATION FUNCTION AND LAST LAYER ACTIVATION FUNCIONS (GENERALLY: HIDDEN ARE RELU OR TANH)\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            architecture:list=[2,3,3,1], \n",
    "            hidden_layers_activation_function=\"relu\", \n",
    "            output_layer_activation_function=\"sigmoid\",\n",
    "            activation_function:str=\"sigmoid\", \n",
    "            learning_rate=0.01,\n",
    "            ):\n",
    "\n",
    "        self.architecture = architecture\n",
    "        self.cache = dict()\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # weights initialized using random numbers from normal distribution\n",
    "        self.weights = [\n",
    "            np.random.randn(architecture[x+1], architecture[x])\n",
    "            for x in range(len(architecture)-1)\n",
    "            ]\n",
    "        \n",
    "        # biases initialized using zeros for each neuron\n",
    "        self.biases = [\n",
    "            np.zeros((architecture[x+1], 1))\n",
    "            for x in range(len(architecture)-1)\n",
    "        ]\n",
    "        \n",
    "        # activation functions supported\n",
    "        activation_functions_available = {\n",
    "            \"relu\": lambda x: np.maximum(0, x),\n",
    "            \"tanh\": np.tanh,\n",
    "            \"sigmoid\": lambda x: ((1) / (1 + np.exp(-x)))\n",
    "        }\n",
    "\n",
    "        if hidden_layers_activation_function.lower() in activation_functions_available.keys() and output_layer_activation_function.lower() in activation_functions_available.keys():\n",
    "            self.hidden_layers_activation_function = activation_functions_available[hidden_layers_activation_function.lower()]\n",
    "            self.output_layer_activation_function = activation_functions_available[output_layer_activation_function.lower()]\n",
    "        else:\n",
    "            raise NameError(\"Activation function not supported\")\n",
    "\n",
    "# =================\n",
    "        # validation of the activation function\n",
    "        if activation_function.lower() in [\"relu\", \"tanh\", \"sigmoid\"]:\n",
    "            self.activation_function = activation_functions_available[activation_function.lower()]\n",
    "        else: \n",
    "            raise NameError(\"Activation Function not supported\")\n",
    "# =================\n",
    "\n",
    "        derivate_activation_function = {\n",
    "            # relu'(Z) = 1 if Z > 0 else 0\n",
    "            \"relu\": lambda Z: (Z > 0).astype(float),\n",
    "            # tanh'(Z) = 1 - tanh(Z)²\n",
    "            \"tanh\": lambda Z: 1 - np.tanh(Z)**2,\n",
    "            # sig'(Z) = sig(Z) * (1 - sig(Z))\n",
    "            \"sigmoid\": lambda Z: ((1) / (1 + np.exp(-Z))) * (1 - ((1) / (1 + np.exp(-Z))))\n",
    "        }\n",
    "    \n",
    "        self.derivate_activation_function = derivate_activation_function[activation_function.lower()]\n",
    "\n",
    "        \n",
    "    def input_data(self, data, y):\n",
    "        # TODO validation\n",
    "        self.data = data\n",
    "        self.y = y\n",
    "        self.num_instances = len(self.data)\n",
    "\n",
    "    def _forward_propagation(self):\n",
    "        \"\"\"\n",
    "        Basic formula:\n",
    "        Z[l] = W[l] A[l-1] + b[l]\n",
    "        A[l] = g(Z[l])\n",
    "        \n",
    "        Where:\n",
    "            - l: Current Layer\n",
    "            - W: Weights\n",
    "            - A: Activation Vector\n",
    "            - b: Biases\n",
    "            - g: Activation Function\n",
    "        \"\"\"\n",
    "        # dict to save the values of each layer\n",
    "        A = self.data\n",
    "        self.cache[\"A0\"] = A\n",
    "        for layer_idx in range(len(self.architecture)-1):\n",
    "            Z = self.weights[layer_idx] @ A + self.biases[layer_idx]\n",
    "            A = self.activation_function(Z)\n",
    "            self.cache[f\"A{layer_idx + 1}\"] = A\n",
    "            self.cache[f\"Z{layer_idx + 1}\"] = Z\n",
    "        y_hat = A\n",
    "        return y_hat\n",
    "    \n",
    "    def _calculate_loss(self, y_hat):\n",
    "        \"\"\"\n",
    "        Use of cross entropy to calculate the loss\n",
    "\n",
    "        For a single example:\n",
    "            - L(y_hat, y) = -(y * log y_hat + (1 - y) * log(1 - y_hat))\n",
    "\n",
    "        For all training samples:\n",
    "            - C = (1 / m) * sum(L(y_hat, y))\n",
    "        \"\"\"\n",
    "        \n",
    "        # original data and prediction\n",
    "        y = self.y\n",
    "        # y_hat = self._forward_propagation()\n",
    "\n",
    "        # loss calculation based on the matrices\n",
    "        prediction_losses = -((y * np.log(y_hat)) + (1 - y) * np.log(1 - y_hat))\n",
    "\n",
    "        # num of entities extracted to get the global loss\n",
    "        y_total = y_hat.reshape(-1).shape[0]\n",
    "\n",
    "        # global loss\n",
    "        losses_sum = (1 / y_total) * np.sum(prediction_losses, axis=1)\n",
    "\n",
    "        return np.sum(losses_sum)\n",
    "\n",
    "    def _backpropagation(self, y_hat, y_real, m):\n",
    "        \"\"\"ULTIMO LAYER: DERIVADA DA FUNCAO DE ATIVAÇÃO ISOLADA\"\"\"\n",
    "        gradient_W = [None] * len(self.weights)\n",
    "        gradient_b = [None] * len(self.weights)\n",
    "\n",
    "        # last layer dZ\n",
    "        # only one that uses explicitly the derivative dC/dZ\n",
    "        # predicted value - real value * scalar factor (1/m)\n",
    "        # TODO: THIS ONE ISALL  BOUT THE SIGMOID FUNCTION, ADAPT IT TO RECEIVE THE RELU DERIVATIVE + TANH DERIVATIVE\n",
    "        dZ = (1/m) * (y_hat - y_real)\n",
    "\n",
    "        for layer_idx in reversed(range(len(self.weights))):\n",
    "            \n",
    "            # correto (dW = dZ * A^t[l-1])\n",
    "            W = self.cache[f\"A{layer_idx}\"]\n",
    "            dW = dZ @ W.T\n",
    "            \n",
    "            #correto (db = sum(dZ))\n",
    "            db = np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "            # saving the weights and biases gradients\n",
    "            gradient_W[layer_idx] = dW\n",
    "            gradient_b[layer_idx] = db\n",
    "\n",
    "            if layer_idx > 0:\n",
    "                # correto - derivada da camada anterior (w^t * dz)\n",
    "                dA_back = self.weights[layer_idx]\n",
    "                dA_back = dA_back.T @ dZ\n",
    "\n",
    "                dZ = dA_back * self.derivate_activation_function(self.cache[f\"Z{layer_idx}\"])\n",
    "\n",
    "        for layer_idx in range(len(self.weights)):\n",
    "            # weights and biases adaptation using gradient descent\n",
    "            # theta = theta - learning rate * slope     (derivative)\n",
    "            self.weights[layer_idx] -= self.learning_rate * gradient_W[layer_idx]\n",
    "            self.biases[layer_idx] -= self.learning_rate * gradient_b[layer_idx]\n",
    "\n",
    "    def train(self, max_iterations:int=10000, min_loss_difference:float=None, file_to_save_weights_and_biases:str=None):\n",
    "        \n",
    "        for _ in range(max_iterations):\n",
    "            y_hat = self._forward_propagation()\n",
    "            loss = self._calculate_loss(\n",
    "                y_hat=y_hat\n",
    "                )\n",
    "            self._backpropagation(\n",
    "                y_hat=y_hat,\n",
    "                y_real=self.y,\n",
    "                m=self.num_instances\n",
    "                )\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, data):\n",
    "        A = data\n",
    "        for layer_idx in range(len(self.architecture)-1):\n",
    "            Z = self.weights[layer_idx] @ A + self.biases[layer_idx]\n",
    "            A = self.activation_function(Z)\n",
    "        y_hat = A\n",
    "        return y_hat #(y_hat>0.5).astype(float) can be used for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32d1346e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = neural_network()\n",
    "\n",
    "def prepare_data():\n",
    "  X = np.array([\n",
    "      [150, 70],\n",
    "      [254, 73],\n",
    "      [312, 68],\n",
    "      [120, 60],\n",
    "      [154, 61],\n",
    "      [212, 65],\n",
    "      [216, 67],\n",
    "      [145, 67],\n",
    "      [184, 64],\n",
    "      [130, 69]\n",
    "  ])\n",
    "  y = np.array([0,1,1,0,0,1,1,0,1,0])\n",
    "  m = 10\n",
    "  A0 = X.T\n",
    "  Y = y.reshape(1, m)\n",
    "\n",
    "  return A0, Y, m\n",
    "\n",
    "A0, Y, m = prepare_data()\n",
    "\n",
    "nn.input_data(A0, Y)\n",
    "\n",
    "nn.train()\n",
    "\n",
    "nn.predict(data=np.array([150,70]).reshape(2,1))\n",
    "# print(nn.weights)\n",
    "# print(nn.biases)\n",
    "\n",
    "# y_hat = nn._forward_propagation()\n",
    "# loss1 = nn._calculate_loss(y_hat)\n",
    "\n",
    "# nn._backpropagation(y_hat, Y, m)\n",
    "\n",
    "# y_hat2 = nn._forward_propagation()\n",
    "# loss2 = nn._calculate_loss(y_hat2)\n",
    "\n",
    "# print(nn.weights)\n",
    "# print(nn.biases)\n",
    "\n",
    "\n",
    "# print(loss1, loss2)\n",
    "\n",
    "# for i in range(100000):\n",
    "#   y_hat = nn._forward_propagation()\n",
    "#   print(nn._calculate_loss(y_hat))\n",
    "#   nn._backpropagation(y_hat, Y, m)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fde24ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
